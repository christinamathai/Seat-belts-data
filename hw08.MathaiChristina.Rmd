---
title: "R Notebook"
output:
  pdf_document: 
    latex_engine: xelatex
  html_notebook: default
---

Problem 1: Seat belts
1. For this problem we are trying to test the effectiveness of mandatory seat belts usage laws in reducing traffic mortality. The independent variable (Y) is fatalityrate. Run all your regressions using the lm parameter. You need to download the seatbelts dataset to complete this part.
```{r}
seat_belts_data <- as.data.frame(read.csv(file = "~/Downloads/seatbelts/seatbelts.csv", header = T, stringsAsFactors = F))
str(seat_belts_data)
```


1.1 Run an interpret the bivariate regression of fatalityrate on primary (this is a binary variable that indicates the primary enforcement of seat belt laws).
```{r}
lm1 <- lm(formula = fatalityrate ~ primary, data = seat_belts_data)
summary(lm1)
```
Interpretation: From the bivariate regression of fatalityrate on primary we can see that the primary enforcement of seat belt laws has a negative effect on the fatalityrate. In this, the F statistic is not significant (p = 0.0117) which indicates that the fit of intercept of the model and that of the current model is same. 

1.2 Create a correlation matrix for the entire dataset using the cor command - exclude non-numeric variables -. Do you think that the exogeneity assumption may not be satisfied for the previous regression? (Explain)
```{r}
library(corrplot)
library(kableExtra)
seat_belts_data$sb_useage[is.na(seat_belts_data$sb_useage)] <- mean(seat_belts_data$sb_useage[!is.na(seat_belts_data$sb_useage)])
seatbelts_numeric <- seat_belts_data[ ,sapply(seat_belts_data, is.numeric)]
cor_plot <- cor(seatbelts_numeric, method = "pearson")
kable(cor_plot, caption = "Correlation Matrix")
corrplot(cor_plot, order = "hclust", addrect = 2, title = "Correlation Plot")
```

Interpretation: Exogenity is an assumption made in regression analysis wherein it states that an independent variable X is not dependant on the dependent variable Y. However this does not indicate that there is no connection. Since Y is a dependent variable it is dependent to the variable X and the error term (Source: https://www.statisticshowto.datasciencecentral.com/exogeneity/). Exogenity is required because if the independent variable is not independent of the error term and the dependant variable Y, then the regression coefficients are not consistant. Hence, exogenity assumption may not be satisfied for the previous regression.

1.3 Using the dataset provided, run a set of 3 additional multiple regressions by sequentially adding other variables that you think are relevant in the model. For each regression (1) Argue why you add the particular additional variable, (2) interpret the parameters, (3) the R2 and adjusted R2, and, (4) the F-statistic.
```{r}
mr1 <- lm(formula = fatalityrate ~ sb_useage + speed65 + ba08 + income, data = seat_belts_data)
summary(mr1)
```
Interpretation: In this multiple regression, I chose seat belt usage, blood alcholo limit, income and speed of 65 mile per hour limit. Based on the correlation plot, these variables are positively correlated
R2 and adjusted R2: The R2 is 0.5104 and the adjusted R2 is 0.5078. From this we can say that there is 51% of variation in the fatality rate and by adding the 4 other variables has caused the variation of adjusted R2 to 50.78%
F-statistic: The p-value : < 2.2e-16 tells us that the model is better than the intercept only model and that one of the variables has an effect on fatality rate. 

2. College on educational attainment.
For this problem we are going to explore the effect of distance from college on educational attain- ment. The independent variable (Y) is years of completed education ed. All the estimated regression parameters for this part should be computed using linear algebra - see lesson 8.2 -. Also, any statistic (F-statistic or R2) should be computed manually and without the use of the lm command - you can use the command to verify your work -. You need to download the collegeDistance dataset to complete this part.
```{r}
library("readxl")
college_dist <- as.data.frame(read_excel("~/CollegeDistance.xls"))
str(college_dist)
```

2.1 Run an interpret the bivariate regression of ed on dist (distance to college). What’s the estimated slope?
```{r}
mat1 <- cbind(rep(1, nrow(college_dist)), college_dist$dist)
#(X′X) − 1 X′Y = β
b1 <- solve(t(mat1) %*% mat1 )  %*%  t(mat1) %*% college_dist$ed
b1
```
From the above derivation, the slope of the bivariate regression of ed on dist is -0.0733

2.2 Now, run a multiple regression of ed on dist but also include: bytest, female, black, hispanic, incomehi, ownhome, dadcoll, momcoll, cue80, and, stwmfg80. What is the estimated effect of ed on dist? Compare your result to the previous estimation. Explain why the effects may differ.
```{r}
mat2 <- as.matrix(cbind(college_dist$dist, college_dist$bytest, college_dist$female, college_dist$black, college_dist$hispanic, college_dist$incomehi, college_dist$ownhome, college_dist$dadcoll, college_dist$momcoll, college_dist$cue80, college_dist$stwmfg80))
mat2 <- cbind(1, mat2)
#(X′X) − 1 X′Y = β
b2 <- solve(t(mat2) %*% mat2 )  %*%  t(mat2) %*% college_dist$ed
b2

mr2 <- lm(formula = ed ~ dist + bytest + female + black + hispanic + incomehi + ownhome + dadcoll + momcoll + cue80 + stwmfg80, data = college_dist)
summary(mr2)
```
Interpretation: Based on the previous estimation, the effects may differ due to the addition of several variables that contravene the exogenity assumption wherein these variables can cause variations on the dependent variable Y.

2.3 Compute the R2 and the adjusted R2 for both regressions and interpret its significance. Which measure of goodness of fit you prefer in each regression?
R2
$$ R ^ 2 = \frac {TSS - SSE} {TSS}$$
$$TSS = \sum_i(y_i - \bar y) ^ 2 $$
$$SSE = \sum_i (y_i - \bar y_i) ^ 2 $$
Adjusted R2
$$ adjusted \space R^2 = \frac {TSS/df_t - SSE/df_e} {TSS/df_t}$$
```{r}
#Model 1
#R2
lm2 <- lm(formula = ed ~ dist, data = college_dist)
ypred <- predict(lm2)
y <- college_dist$ed
tss <- sum((y - mean(y)) ^ 2) 
sse <- sum((y - ypred) ^ 2)
r1 <- (tss-sse) / tss
r1
#adjusted R2
n <- length(y)
k <- 1
dft <- n - 1
dfe <- n - k - 1
(tss / dft - sse / dfe) / (tss / dft)
```


```{r}
#Model 2
#R2
lm3 <- lm(formula = ed ~ dist + bytest + female + black + hispanic + incomehi + ownhome + dadcoll + momcoll + cue80 + stwmfg80, data = college_dist)
ypred <- predict(lm3)
y <- college_dist$ed
tss <- sum((y - mean(y)) ^ 2) 
sse <- sum((y - ypred) ^ 2)
r2 <- (tss-sse) / tss
r2
#adjusted R2
n <- length(y)
k1 <- ncol(mat2)-1
dft <- n - 1
dfe <- n - k1 - 1
(tss / dft - sse / dfe) / (tss / dft)
```
Interpretation: Adjusted R2 is a better measure of goodness of fit since it controls the number of variables in this model. Unlike bivariate model where the use of R2 or adjusted R2 does not matter, in multivariate models we need to depend on the other independent variables too.As the number of variables increases, the adjustred R2 is a better measure of goodness of fit. However, in this case, we can use both. 

2.4 Bob is a non-hispanic black male. His high school was 20 miles from the nearest college. His base year composite score (bytest) was 58. His family income in 1980 was $26, 000, and his family owned a house. His mother attended college, but his father did not. The unemployment rate in his county was 7.5%, and the state average manufacturing hourly wage was $9.75. Predict Bob’s years of completed schooling using both regressions and compare the results. Which result you prefer? (Explain)
```{r}
n1 <- c(1, 2) %*% b1 
n1
data1 <- c(2, 58, 0, 1, 0, 1, 1, 0, 1, 7.5, 9.75)
c(1, data1) %*% b2
```
Interpretation: From the bivariate model it predicts that Bob will complete his schooling in 13.8 years and in the multivariate model it predicts that he will complete his schooling in 15 years. I prefer the second model since the R2 is higher.

2.5 Test if all the parameters of the model are simultaneously equal to zero.
$$ F -test : F = \frac {R^2/k} {(1-R^2)/ n- k - 1}$$
```{r}
f <- (r2 / k1) / ((1 - r2) / (n - k1 - 1))
f
#p-value
pf(f, k1, (n - k1 - 1), lower.tail = F)
```
Interpretation: The p-value is less than 0.05 we reject the null hypothesis where all the parameters of the model are simultaneously equal to zero